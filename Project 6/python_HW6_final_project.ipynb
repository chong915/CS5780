{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h2>Final Project: Identifying Trump's Tweets</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"white_house.jpg\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>The goal is to classify the device that Trump uses to write each tweet with. It's been hypothesized that President Trump tweets only from his android phone and that someone else (his staff) tweets from his account using an iPhone. Analyze the text of the tweet as well as other contextual information to predict where each tweet came from. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rules</h3>\n",
    "\n",
    "<p> Rules of the competition: You may use any techniques you've learned in class including any open source implementations in packages such as scikit-learn, tensorflow, or pre-trained models. If you use any open source implementations, <b>please cite them in your comments</b>. The sharing of personal code between teams is strictly not allowed. Additionally obtaining a copy of the labeled test set through any means is expressly forbidden. </p>\n",
    "\n",
    "<p><b>NOTE: You are only allowed 10 submissions for this project. Please use them carefully. We will use your 10th and final submission (not be the best one) for grading.</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grading</h3>\n",
    "\n",
    "<p>There are two baselines we have implemented. <code>Baseline 1 = 0.7</code> and <code>Baseline 2 = 0.82</code>. If you beat the first baseline, you will 90 points. If you beat the second baseline, you'll get 100 points.</p>\n",
    "<p>The top 30 teams on the leaderboard will receive an extra 5 bonus points.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately our initially high accuracy was due to overfitting. When not explicitly specifying a kernel in sklearn's SVM function, it uses an rbf kernel. Using k-fold cross-validation our accuracy dropped from 98% to 67%. A linear kernel performs slightly better with a little over 70% accuracy.\n",
    "\n",
    "### To do (added by Martin)\n",
    "\n",
    "Implement multiple learning algorithms.\n",
    "\n",
    "Optimize functions.\n",
    "\n",
    "Feature ideas:\n",
    "- Average number of words per sentence\n",
    "- Average word length\n",
    "- Are URLs or emojis in the tweet?\n",
    "- Tokenize words using word2vec or bag of words\n",
    "\n",
    "### What has been done\n",
    "\n",
    "Implemented features:\n",
    "- Number of sentences per tweet\n",
    "- Numbers of characters per tweet\n",
    "- Number of characters per sentence\n",
    "- Day of the week\n",
    "- time of the day\n",
    "- number of capital letters\n",
    "- Sentiment analysis -> strangely sentiment analysis reduces the performance!\n",
    "- Number of punctuation symbols (...only exclamation points, but we can scale up)\n",
    "\n",
    "Implemented sklearn's SVM as learning algorithm.\n",
    "\n",
    "Implement k-fold cross validation. Added by Cole: grid search with cross-validation evaluating C and kernel parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Cole\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#</GRADED>\n",
    "\n",
    "## include your imports as necessary and cite open-source implementations appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def read_files(train_file):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    df_X : pandas data frame of training data\n",
    "    Y    : numpy array of labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(train_file, index_col=0)\n",
    "    df_X = df[df.columns[0:17]]\n",
    "    Y = np.array(df['label'])\n",
    "        \n",
    "    return df_X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Training Data </h3>\n",
    "\n",
    "<p> Take a look at the file <code>train.csv</code>. Here are the first 4 tweets in the train dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    favorited  favoriteCount  truncated  replyToSID                id.1  \\\n",
      "id                                                                        \n",
      "0       False          14207      False         NaN  752668000000000000   \n",
      "1       False           9666      False         NaN  752628000000000000   \n",
      "2       False          25531      False         NaN  752619000000000000   \n",
      "3       False          28850      False         NaN  752591000000000000   \n",
      "4       False          12567      False         NaN  752472000000000000   \n",
      "\n",
      "    replyToUID  retweetCount  isRetweet  retweeted  longitude  \\\n",
      "id                                                              \n",
      "0          NaN          5256      False      False        NaN   \n",
      "1          NaN          3432      False      False        NaN   \n",
      "2          NaN          8810      False      False        NaN   \n",
      "3          NaN          9112      False      False        NaN   \n",
      "4          NaN          4144      False      False        NaN   \n",
      "\n",
      "               ...              \\\n",
      "id             ...               \n",
      "0              ...               \n",
      "1              ...               \n",
      "2              ...               \n",
      "3              ...               \n",
      "4              ...               \n",
      "\n",
      "    text_Senior United States District Judge Robert E. Payne today ruled in favor of Trump campaign delegates who had argued..https://t.co/qVwfjgCHU7  \\\n",
      "id                                                                                                                                                      \n",
      "0                                                   1                                                                                                   \n",
      "1                                                   0                                                                                                   \n",
      "2                                                   0                                                                                                   \n",
      "3                                                   0                                                                                                   \n",
      "4                                                   0                                                                                                   \n",
      "\n",
      "    text_Speech on Veterans' Reform:  https://t.co/XB7RMwesMK  \\\n",
      "id                                                              \n",
      "0                                                   0           \n",
      "1                                                   1           \n",
      "2                                                   0           \n",
      "3                                                   0           \n",
      "4                                                   0           \n",
      "\n",
      "    text_Thoughts and prayers with the victims; and their families- along with everyone at the Berrien County Courthouse in St. Joseph; Michigan.  \\\n",
      "id                                                                                                                                                  \n",
      "0                                                   0                                                                                               \n",
      "1                                                   0                                                                                               \n",
      "2                                                   0                                                                                               \n",
      "3                                                   1                                                                                               \n",
      "4                                                   0                                                                                               \n",
      "\n",
      "    created_7/11/2016 11:57  created_7/11/2016 19:51  created_7/11/2016 21:40  \\\n",
      "id                                                                              \n",
      "0                         0                        0                        0   \n",
      "1                         0                        0                        0   \n",
      "2                         0                        0                        1   \n",
      "3                         0                        1                        0   \n",
      "4                         1                        0                        0   \n",
      "\n",
      "    created_7/11/2016 22:18  created_7/12/2016 0:56  \\\n",
      "id                                                    \n",
      "0                         0                       1   \n",
      "1                         1                       0   \n",
      "2                         0                       0   \n",
      "3                         0                       0   \n",
      "4                         0                       0   \n",
      "\n",
      "    statusSource_<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>  \\\n",
      "id                                                                                                    \n",
      "0                                                   1                                                 \n",
      "1                                                   1                                                 \n",
      "2                                                   1                                                 \n",
      "3                                                   1                                                 \n",
      "4                                                   1                                                 \n",
      "\n",
      "    screenName_realDonaldTrump  \n",
      "id                              \n",
      "0                            1  \n",
      "1                            1  \n",
      "2                            1  \n",
      "3                            1  \n",
      "4                            1  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df_X_train, Y_train = read_files('train.csv')\n",
    "df_X_train[:4]\n",
    "\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#data = df_X_train.values\n",
    "\n",
    "#data[:, [1, 2]] = encoder.fit_transform(data[:, [1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train and Classify </h3>\n",
    "\n",
    "<p> Implement <code>train_and_classify</code>. It should extract feature vectors from the given pandas dataframes. Train a model and return the labels of the test data. The feature vectors and models to use are up to you to decide.</p>\n",
    "\n",
    "<p><b>Your final score will be determined by executing <code>train_and_classify</code> with the provided training set for training and a hidden test set for classification. We will then evaluate the accuracy of your output.</b></p>\n",
    "<p><b>NOTE: Please limit your training time to 10 minutes.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_length(df_X_train):\n",
    "    # the tweets themselves are in the zero-th column. Extract from dataframe\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    length = tweets.str.len()\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(string):\n",
    "    '''\n",
    "    Splits a tweet on \". \" such that sentences are separated from each other.\n",
    "    Does not split on \".\" so that periods in URLs are not misunderstood as the end of a sentence\n",
    "    \n",
    "    We should add additional characters to split on like exclamation marks and question marks. We could also separate out URLs\n",
    "    so we can split on punctuation without worrying about spaces.\n",
    "    '''\n",
    "    return string.split(\". \")\n",
    "\n",
    "def extract_number_of_sentences(df_X_train):\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    splitted_tweets = tweets.apply(split_sentences)\n",
    "    n_sentences = splitted_tweets.apply(len)\n",
    "    \n",
    "    return n_sentences\n",
    "\n",
    "def extract_number_of_characters_per_sentence(df_X_train):    \n",
    "    def average_characters_per_string(list_of_strings):\n",
    "        return np.mean(list(map(len,list_of_strings)))\n",
    "\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    splitted_tweets = tweets.apply(split_sentences)\n",
    "    n_characters_per_sentence = splitted_tweets.apply(average_characters_per_string)\n",
    "    \n",
    "    return n_characters_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weekday(df_X_train):\n",
    "    df_X_train['created'] = pd.to_datetime(df_X_train['created'])\n",
    "    day_of_week = df_X_train['created'].dt.dayofweek.rename('day_of_week')\n",
    "    \n",
    "    return day_of_week\n",
    "\n",
    "def extract_time_of_day(df_X_train):\n",
    "    df_X_train['created'] = pd.to_datetime(df_X_train['created'])\n",
    "    hour = df_X_train['created'].dt.hour.rename('hour')\n",
    "    \n",
    "    return hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uppercase_letters(string):\n",
    "    uppercase = filter(str.isupper, string) \n",
    "    return len(list(uppercase))\n",
    "\n",
    "def extract_number_of_uppercase_letters(df_X_train):\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    n_uppercase_characters = tweets.apply(count_uppercase_letters)\n",
    "    \n",
    "    return n_uppercase_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(string):\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # return the compound (index 3) sentiment score on a string\n",
    "    return list(sid.polarity_scores(string).values())[3]\n",
    "\n",
    "def extract_sentiment(df_X_train):\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    \n",
    "    return tweets.apply(find_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exclamation_points(df_X_train):\n",
    "    n_exclamation_points = df_X_train['text'].str.count('!')\n",
    "    return n_exclamation_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def train_and_classify(df_X_train, Y_train, df_X_test):\n",
    "    \"\"\"\n",
    "    Extracts features from df_X_train. Train a model\n",
    "    on training data and training labels (Y_train).\n",
    "    Predict the labels of df_X_test.\n",
    "    \n",
    "    df_X_train : pandas data frame of training data\n",
    "    Y_train    : numpy array of labels for training data\n",
    "    df_X_test  : pandas data frame of test data\n",
    "    \n",
    "    Output:\n",
    "    Y_test : numpy array of labels for test data\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.concat([df_X_train, df_X_test], axis = 0)\n",
    "    \n",
    "    ## fill in code here\n",
    "    def extract_feature_vec(df_X):\n",
    "        # extracts feature vectors\n",
    "        features = []\n",
    "        \n",
    "        features.append(extract_length(df_X))\n",
    "        features.append(extract_number_of_sentences(df_X))\n",
    "        features.append(extract_number_of_characters_per_sentence(df_X))\n",
    "        features.append(extract_weekday(df_X))\n",
    "        features.append(extract_time_of_day(df_X))\n",
    "        features.append(extract_number_of_uppercase_letters(df_X))\n",
    "        features.append(extract_sentiment(df_X))\n",
    "        features.append(extract_exclamation_points(df_X))\n",
    "        \n",
    "        return pd.concat(features, axis=1)\n",
    "    \n",
    "    df = extract_feature_vec(df)\n",
    "    \n",
    "    dummies = pd.get_dummies(df, columns = ['day_of_week', 'hour'])\n",
    "    \n",
    "    df = df.drop(columns = ['day_of_week', 'hour'])\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_array = scaler.fit_transform(df)\n",
    "    \n",
    "    array = np.concatenate([scaled_array, dummies.values], axis = 1)\n",
    "    \n",
    "    X_train = array[:df_X_train.shape[0], :]\n",
    "    X_test = array[df_X_train.shape[0]:, :]\n",
    "    \n",
    "    # create and train model (consider doing k-fold cross validation as well)\n",
    "    svc = svm.SVC(kernel = 'linear')\n",
    "    parameters = {'C':(2.0**np.linspace(-10, 4, 5))}\n",
    "    clf = GridSearchCV(svc, parameters, cv = 3)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    # evaulate model\n",
    "    Y_test = clf.predict(X_test) \n",
    "\n",
    "    return Y_test\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluation</h3>\n",
    "\n",
    "<p>Below is some code to see your accuracy when trained and tested on the training data set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n",
      "1089\n",
      "accurary: 77.13%\n"
     ]
    }
   ],
   "source": [
    "# evalulate and classify on training set\n",
    "Y_pred = train_and_classify(df_X_train, Y_train, df_X_train)\n",
    "\n",
    "def accuracy(Y_pred, Y_true):\n",
    "    return np.sum(Y_pred == Y_true) / Y_pred.shape[0]\n",
    "\n",
    "acc = accuracy(Y_pred, Y_train)\n",
    "print('accurary: ' + str(round(acc * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "980\n",
      "981\n",
      "981\n",
      "Accuracy:  72.81 +/- 1.36 %\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=False)\n",
    "acc = np.zeros(k)\n",
    "i=0\n",
    "  \n",
    "for train_index, test_index in kf.split(df_X_train):\n",
    "    \n",
    "    df_X_train_selection = df_X_train.loc[train_index]\n",
    "    Y_train_selection = Y_train[train_index]\n",
    "    \n",
    "    df_X_test_selection = df_X_train.loc[test_index]\n",
    "    Y_test_selection = Y_train[test_index]\n",
    "    \n",
    "    Y_pred = train_and_classify(df_X_train_selection, Y_train_selection, df_X_test_selection)\n",
    "    \n",
    "    acc[i] = accuracy(Y_pred, Y_test_selection)\n",
    "    i+=1\n",
    "    \n",
    "print(\"Accuracy: \", round(np.mean(acc)*100,2), \"+/-\", round(np.std(acc)*100/np.sqrt(k),2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
