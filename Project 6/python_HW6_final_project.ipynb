{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h2>Final Project: Identifying Trump's Tweets</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"white_house.jpg\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>The goal is to classify the device that Trump uses to write each tweet with. It's been hypothesized that President Trump tweets only from his android phone and that someone else (his staff) tweets from his account using an iPhone. Analyze the text of the tweet as well as other contextual information to predict where each tweet came from. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rules</h3>\n",
    "\n",
    "<p> Rules of the competition: You may use any techniques you've learned in class including any open source implementations in packages such as scikit-learn, tensorflow, or pre-trained models. If you use any open source implementations, <b>please cite them in your comments</b>. The sharing of personal code between teams is strictly not allowed. Additionally obtaining a copy of the labeled test set through any means is expressly forbidden. </p>\n",
    "\n",
    "<p><b>NOTE: You are only allowed 10 submissions for this project. Please use them carefully. We will use your 10th and final submission (not be the best one) for grading.</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grading</h3>\n",
    "\n",
    "<p>There are two baselines we have implemented. <code>Baseline 1 = 0.7</code> and <code>Baseline 2 = 0.82</code>. If you beat the first baseline, you will 90 points. If you beat the second baseline, you'll get 100 points.</p>\n",
    "<p>The top 30 teams on the leaderboard will receive an extra 5 bonus points.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately our initially high accuracy was due to overfitting. When not explicitly specifying a kernel in sklearn's SVM function, it uses an rbf kernel. Using k-fold cross-validation our accuracy dropped from 98% to 67%. A linear kernel performs slightly better with a little over 70% accuracy.\n",
    "\n",
    "### To do (added by Martin)\n",
    "\n",
    "Implement multiple learning algorithms.\n",
    "\n",
    "Optimize functions.\n",
    "\n",
    "Feature ideas:\n",
    "- Average number of words per sentence\n",
    "- Average word length\n",
    "- Are URLs or emojis in the tweet?\n",
    "- Tokenize words using word2vec or bag of words\n",
    "\n",
    "### What has been done\n",
    "\n",
    "Implemented features:\n",
    "- Number of sentences per tweet\n",
    "- Numbers of characters per tweet\n",
    "- Number of characters per sentence\n",
    "- Day of the week\n",
    "- time of the day\n",
    "- number of capital letters\n",
    "- Sentiment analysis -> strangely sentiment analysis reduces the performance!\n",
    "- Number of punctuation symbols (...only exclamation points, but we can scale up)\n",
    "\n",
    "Implemented sklearn's SVM as learning algorithm.\n",
    "\n",
    "Implement k-fold cross validation. Added by Cole: grid search with cross-validation evaluating C and kernel parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#</GRADED>\n",
    "\n",
    "## include your imports as necessary and cite open-source implementations appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def read_files(train_file):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    df_X : pandas data frame of training data\n",
    "    Y    : numpy array of labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(train_file, index_col=0)\n",
    "    df_X = df[df.columns[0:17]]\n",
    "    Y = np.array(df['label'])\n",
    "        \n",
    "    return df_X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Training Data </h3>\n",
    "\n",
    "<p> Take a look at the file <code>train.csv</code>. Here are the first 4 tweets in the train dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id.1</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior United States District Judge Robert E. ...</td>\n",
       "      <td>False</td>\n",
       "      <td>14207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/12/2016 0:56</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752668000000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>5256</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Speech on Veterans' Reform:  https://t.co/XB7R...</td>\n",
       "      <td>False</td>\n",
       "      <td>9666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/11/2016 22:18</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752628000000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>3432</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great poll- Florida! Thank you! https://t.co/4...</td>\n",
       "      <td>False</td>\n",
       "      <td>25531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/11/2016 21:40</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752619000000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>8810</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thoughts and prayers with the victims; and the...</td>\n",
       "      <td>False</td>\n",
       "      <td>28850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/11/2016 19:51</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752591000000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>9112</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  favorited  \\\n",
       "id                                                                 \n",
       "0   Senior United States District Judge Robert E. ...      False   \n",
       "1   Speech on Veterans' Reform:  https://t.co/XB7R...      False   \n",
       "2   Great poll- Florida! Thank you! https://t.co/4...      False   \n",
       "3   Thoughts and prayers with the victims; and the...      False   \n",
       "\n",
       "    favoriteCount replyToSN          created  truncated  replyToSID  \\\n",
       "id                                                                    \n",
       "0           14207       NaN   7/12/2016 0:56      False         NaN   \n",
       "1            9666       NaN  7/11/2016 22:18      False         NaN   \n",
       "2           25531       NaN  7/11/2016 21:40      False         NaN   \n",
       "3           28850       NaN  7/11/2016 19:51      False         NaN   \n",
       "\n",
       "                  id.1  replyToUID  \\\n",
       "id                                   \n",
       "0   752668000000000000         NaN   \n",
       "1   752628000000000000         NaN   \n",
       "2   752619000000000000         NaN   \n",
       "3   752591000000000000         NaN   \n",
       "\n",
       "                                         statusSource       screenName  \\\n",
       "id                                                                       \n",
       "0   <a href=\"http://twitter.com/download/iphone\" r...  realDonaldTrump   \n",
       "1   <a href=\"http://twitter.com/download/iphone\" r...  realDonaldTrump   \n",
       "2   <a href=\"http://twitter.com/download/iphone\" r...  realDonaldTrump   \n",
       "3   <a href=\"http://twitter.com/download/iphone\" r...  realDonaldTrump   \n",
       "\n",
       "    retweetCount  isRetweet  retweeted  longitude  latitude  label  \n",
       "id                                                                  \n",
       "0           5256      False      False        NaN       NaN     -1  \n",
       "1           3432      False      False        NaN       NaN     -1  \n",
       "2           8810      False      False        NaN       NaN     -1  \n",
       "3           9112      False      False        NaN       NaN     -1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train, Y_train = read_files('train.csv')\n",
    "df_X_train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train and Classify </h3>\n",
    "\n",
    "<p> Implement <code>train_and_classify</code>. It should extract feature vectors from the given pandas dataframes. Train a model and return the labels of the test data. The feature vectors and models to use are up to you to decide.</p>\n",
    "\n",
    "<p><b>Your final score will be determined by executing <code>train_and_classify</code> with the provided training set for training and a hidden test set for classification. We will then evaluate the accuracy of your output.</b></p>\n",
    "<p><b>NOTE: Please limit your training time to 10 minutes.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_length(df_X_train):\n",
    "    # the tweets themselves are in the zero-th column. Extract from dataframe\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    length = tweets.str.len()\n",
    "    return length/max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(string):\n",
    "    '''\n",
    "    Splits a tweet on \". \" such that sentences are separated from each other.\n",
    "    Does not split on \".\" so that periods in URLs are not misunderstood as the end of a sentence\n",
    "    \n",
    "    We should add additional characters to split on like exclamation marks and question marks. We could also separate out URLs\n",
    "    so we can split on punctuation without worrying about spaces.\n",
    "    '''\n",
    "    return string.split(\". \")\n",
    "\n",
    "def extract_number_of_sentences(df_X_train):\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    splitted_tweets = tweets.apply(split_sentences)\n",
    "    n_sentences = splitted_tweets.apply(len)\n",
    "    \n",
    "    return n_sentences/max(n_sentences)\n",
    "\n",
    "def extract_number_of_characters_per_sentence(df_X_train):    \n",
    "    def average_characters_per_string(list_of_strings):\n",
    "        return np.mean(list(map(len,list_of_strings)))\n",
    "\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    splitted_tweets = tweets.apply(split_sentences)\n",
    "    n_characters_per_sentence = splitted_tweets.apply(average_characters_per_string)\n",
    "    \n",
    "    return n_characters_per_sentence/max(n_characters_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weekday(df_X_train):\n",
    "    df_X_train['created'] = pd.to_datetime(df_X_train['created'])\n",
    "    day_of_week = df_X_train['created'].dt.dayofweek\n",
    "    \n",
    "    return day_of_week/max(day_of_week)\n",
    "\n",
    "def extract_time_of_day(df_X_train):\n",
    "    df_X_train['created'] = pd.to_datetime(df_X_train['created'])\n",
    "    hour = df_X_train['created'].dt.hour\n",
    "    \n",
    "    return hour/max(hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uppercase_letters(string):\n",
    "    uppercase = filter(str.isupper, string) \n",
    "    return len(list(uppercase))\n",
    "\n",
    "def extract_number_of_uppercase_letters(df_X_train):\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    n_uppercase_characters = tweets.apply(count_uppercase_letters)\n",
    "    \n",
    "    return n_uppercase_characters/max(n_uppercase_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(string):\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # return the compound (index 3) sentiment score on a string\n",
    "    return list(sid.polarity_scores(string).values())[3]\n",
    "\n",
    "def extract_sentiment(df_X_train):\n",
    "    tweets = df_X_train.iloc[:,0]\n",
    "    \n",
    "    return tweets.apply(find_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exclamation_points(df_X_train):\n",
    "    n_exclamation_points = df_X_train['text'].str.count('!')\n",
    "    return n_exclamation_points/max(n_exclamation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def train_and_classify(df_X_train, Y_train, df_X_test):\n",
    "    \"\"\"\n",
    "    Extracts features from df_X_train. Train a model\n",
    "    on training data and training labels (Y_train).\n",
    "    Predict the labels of df_X_test.\n",
    "    \n",
    "    df_X_train : pandas data frame of training data\n",
    "    Y_train    : numpy array of labels for training data\n",
    "    df_X_test  : pandas data frame of test data\n",
    "    \n",
    "    Output:\n",
    "    Y_test : numpy array of labels for test data\n",
    "    \"\"\"\n",
    "    \n",
    "    ## fill in code here\n",
    "    def extract_feature_vec(df_X):\n",
    "        # extracts feature vectors\n",
    "        features = []\n",
    "        \n",
    "        features.append(extract_length(df_X))\n",
    "        features.append(extract_number_of_sentences(df_X))\n",
    "        features.append(extract_number_of_characters_per_sentence(df_X))\n",
    "        features.append(extract_weekday(df_X))\n",
    "        features.append(extract_time_of_day(df_X))\n",
    "        features.append(extract_number_of_uppercase_letters(df_X))\n",
    "        features.append(extract_sentiment(df_X))\n",
    "        features.append(extract_exclamation_points(df_X))\n",
    "        \n",
    "        return pd.concat(features, axis=1)\n",
    "    \n",
    "    X_train = extract_feature_vec(df_X_train)\n",
    "    X_test  = extract_feature_vec(df_X_test)\n",
    "    \n",
    "    # create and train model (consider doing k-fold cross validation as well)\n",
    "    svc = svm.SVC(kernel = 'linear')\n",
    "    parameters = {'C':(2.0**np.linspace(-10, 4, 5))}\n",
    "    clf = GridSearchCV(svc, parameters, cv = 3)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    # evaulate model\n",
    "    Y_test = clf.predict(X_test) \n",
    "\n",
    "    return Y_test\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluation</h3>\n",
    "\n",
    "<p>Below is some code to see your accuracy when trained and tested on the training data set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurary: 71.63%\n"
     ]
    }
   ],
   "source": [
    "# evalulate and classify on training set\n",
    "Y_pred = train_and_classify(df_X_train, Y_train, df_X_train)\n",
    "\n",
    "def accuracy(Y_pred, Y_true):\n",
    "    return np.sum(Y_pred == Y_true) / Y_pred.shape[0]\n",
    "\n",
    "acc = accuracy(Y_pred, Y_train)\n",
    "print('accurary: ' + str(round(acc * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  70.42 +/- 1.88 %\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=False)\n",
    "acc = np.zeros(k)\n",
    "i=0\n",
    "  \n",
    "for train_index, test_index in kf.split(df_X_train):\n",
    "    \n",
    "    df_X_train_selection = df_X_train.loc[train_index]\n",
    "    Y_train_selection = Y_train[train_index]\n",
    "    \n",
    "    df_X_test_selection = df_X_train.loc[test_index]\n",
    "    Y_test_selection = Y_train[test_index]\n",
    "    \n",
    "    Y_pred = train_and_classify(df_X_train_selection, Y_train_selection, df_X_test_selection)\n",
    "    \n",
    "    acc[i] = accuracy(Y_pred, Y_test_selection)\n",
    "    i+=1\n",
    "    \n",
    "print(\"Accuracy: \", round(np.mean(acc)*100,2), \"+/-\", round(np.std(acc)*100/np.sqrt(k),2), \"%\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
